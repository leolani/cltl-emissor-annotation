{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c4e89b-8f5b-4e5a-beb5-03ec324bd5d3",
   "metadata": {},
   "source": [
    "# Annotating EMISSOR data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ec59f-0297-4b63-95ef-247348a896b5",
   "metadata": {},
   "source": [
    "EMISSOR is used to capture interactions. Each interaction is saved as a unique scenario as JSON files in a subfolder. The scenario can be annotated with interpretations after the interactions. \n",
    "\n",
    "The concepts and structure of EMISSOR are described in Santamaría et al 2021.\n",
    "\n",
    "**Reference**:\n",
    "\n",
    "```Santamaría, Selene Báez, Thomas Baier, Taewoon Kim, Lea Krause, Jaap Kruijt, and Piek Vossen. \"EMISSOR: A platform for capturing multimodal interactions as Episodic Memories and Interpretations with Situated Scenario-based Ontological References.\" In Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR), pp. 56-77. 2021.```\n",
    "\n",
    "We developed several annotation modules to assign:\n",
    "\n",
    "1. The dialogue act of an utterance\n",
    "2. Emotion expressed\n",
    "3. Likelihood of an utterance given the preceding context according to a language model\n",
    "4. Linguistic structures and mentions\n",
    "\n",
    "In the notebook below, we briefly describe each annotation and provide pointers. After annotating, you can analyse the interaction based on the annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324aedde-8ad6-4095-8e27-c1508cdae546",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "The next preparations should be done in a terminal before launching this notebook. \n",
    "So if not done yet, copy the instructions and launch a terminal to carry these out.\n",
    "\n",
    "Open a terminal and do the following preparations:\n",
    "\n",
    "1. Define a Python ```venv``` or Conda environment. Here we use ```annotate``` as the name for the environment:\n",
    "\n",
    "```>python -m venv annotate```\n",
    "```>conda create -n annotate```\n",
    "\n",
    "Activate the environment. See the documentation how to activate.\n",
    "After activation the terminal prompt should be prefixed with ```(annotate)```.\n",
    "\n",
    "2. Install the necessary packages:\n",
    "\n",
    "With ```annotate``` activated install the requirements:\n",
    "\n",
    "```(annotate)>pip install -r requirements.txt```\n",
    "\n",
    "3. Download the spaCy language module:\n",
    "\n",
    "The linguistic annotator uses spaCy for which we need to download a language module in our ```annotate``` environment:\n",
    "\n",
    "```(annotate)>python -m spacy download en_core_web_sm```\n",
    "\n",
    "4. Add jupyter to your environment\n",
    "\n",
    "To make sure that Jupyter will know the ```annotate``` environment, we need to do the following:\n",
    "\n",
    "```(annotate)>python -m ipykernel install --user --name=annotate```\n",
    "\n",
    "5. Launch Jupter in the ```annotate``` environment and open this notebook:\n",
    "\n",
    "```(annotate)>jupyter lab```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c27c2-ba12-4176-b6c2-00ee61513a7a",
   "metadata": {},
   "source": [
    "## Importing the annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d319434-09d8-4630-b7aa-2f84468d07aa",
   "metadata": {},
   "source": [
    "We first need to import the different ```annotators``` defined by the Python script next to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9e1c84-66b6-4b9d-9b11-28a69f5e2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import annotate_emissor_conversation_with_dialogue_acts as dialogacts\n",
    "import annotate_emissor_conversation_with_emotions as emotions\n",
    "import annotate_emissor_conversation_with_llm_likelihood as likelihood\n",
    "import annotate_emissor_conversation_with_text_mentions as mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031162e8-fd00-4a71-9240-c4ec026aad1b",
   "metadata": {},
   "source": [
    "The functions look for subfolders in an emissor folder. These subfolders are treated as scenarios. If no specific scenario is provided, the annotation functions will process all the scenarios. In this notebook, we use the ```emissor``` folder in the ```data``` directory with a single scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9577195-91f7-4096-9e3c-1e484c159042",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissor = \"data/emissor\"\n",
    "scenario = \"14a1c27d-dfd2-465b-9ab2-90e9ea91d214\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd33c2-fc81-431d-8ee4-f02c729c5356",
   "metadata": {},
   "source": [
    "## Annotating utterances with Dialogue Acts\n",
    "\n",
    "Dialogue acts represent the overal intention of an utterance in conversation, such as asing a question, giving an answer, making a statement, etc. We used a dataset and scheme with 23 different dialog acts called MIDAS developed by Yu et al. 2021. We finetuned a crosslingual encoder LLM, ```XLM-RoBERTa``` with the data. The finetuned model is available on Huggingface under ```CLTL/midas-da-xlmroberta```.\n",
    "\n",
    "Reference: ```Yu, Dian, and Zhou Yu. \"MIDAS: A Dialog Act Annotation Scheme for Open Domain HumanMachine Spoken Conversations.\" In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1103-1120. Association for Computational Linguistics, 2021.```\n",
    "\n",
    "The annotator for the dialog acts has the following parameters that can be set:\n",
    "\n",
    "1. ```emissor``` = path to the emissor folder.\n",
    "2. ```scenario``` (optional) = the subfolder in the emissor folder that needs to processed. If omitted all subfolders are processed.\n",
    "3. ```model_name``` (optional) = the name of the model that is used to specificy the provenance of the annotation. If omitted ```MIDAS``` is used as a a name.\n",
    "4.  ```model_path``` (optional) = the name of the model on Huggingface or the path to a local model on your computer. If omitted our model on Huggingface is downloaded and used, which may take a bit of time the first time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b495fda0-0a65-4f40-9ddb-896050fdcfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing emissor data/emissor\n",
      "Using model CLTL/midas-da-xlmroberta\n",
      "Loading MIDAS model... CLTL/midas-da-xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9bf7b8a6b04aa3a29d658162baf12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1a03ab2f2f4fe8aaefea5e9dfa5c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenarios:  ['14a1c27d-dfd2-465b-9ab2-90e9ea91d214']\n",
      "Processing scenario 14a1c27d-dfd2-465b-9ab2-90e9ea91d214\n"
     ]
    }
   ],
   "source": [
    "model = \"CLTL/midas-da-xlmroberta\"\n",
    "dialogacts.main(emissor= emissor, scenario=scenario, model_path=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb069a-fd5f-40dd-9fe5-a4cece7a96f3",
   "metadata": {},
   "source": [
    "The text.json file in the scenario folder is augmented with the dialog act annotations. In EMISSOR, annotations are part of ```mentions```. A mention has a ```segment``` that defines which part of which signal is annotated, and one or more annotations. In the next example, we show the output for the dialogue annotator as an annotation of the full text as a segment: \"I know. I have heard about you before\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39e3aa-6d7d-4b2a-8817-7f997880f6ee",
   "metadata": {},
   "source": [
    "```\n",
    "\"text\": \"I know. I have heard about you before\"\n",
    "      {\n",
    "        \"@context\": {\n",
    "          \"Mention\": \"https://emissor.org#Mention\",\n",
    "          \"id\": \"@id\",\n",
    "          \"segment\": \"https://emissor.org#segment\",\n",
    "          \"annotations\": \"https://emissor.org#annotations\"\n",
    "        },\n",
    "        \"@type\": \"Mention\",\n",
    "        \"id\": \"fd60a4bd-86bf-47e5-bd13-16358835c45d\",\n",
    "        \"segment\": [\n",
    "          {\n",
    "            \"@context\": {\n",
    "              \"Index\": \"https://emissor.org#Index\",\n",
    "              \"id\": \"@id\",\n",
    "              \"start\": \"https://emissor.org#start\",\n",
    "              \"stop\": \"https://emissor.org#stop\",\n",
    "              \"container_id\": {\n",
    "                \"@id\": \"https://emissor.org#container_id\",\n",
    "                \"@type\": \"@id\"\n",
    "              }\n",
    "            },\n",
    "            \"@type\": \"Index\",\n",
    "            \"container_id\": \"7d72105d-52d8-4ca1-b1e8-03d9f3d10d13\",\n",
    "            \"start\": 0,\n",
    "            \"stop\": 37,\n",
    "            \"_py_type\": \"emissor.representation.container-Index\"\n",
    "          }\n",
    "        ],\n",
    "        \"annotations\": [\n",
    "          {\n",
    "            \"@context\": {\n",
    "              \"Annotation\": \"https://emissor.org#Annotation\",\n",
    "              \"id\": \"@id\",\n",
    "              \"type\": \"https://emissor.org#type\",\n",
    "              \"value\": \"https://emissor.org#value\",\n",
    "              \"source\": \"https://emissor.org#source\",\n",
    "              \"timestamp\": \"https://emissor.org#timestamp\"\n",
    "            },\n",
    "            \"@type\": \"Annotation\",\n",
    "            \"type\": \"python-type:cltl.dialogue_act_classification.api.DialogueAct\",\n",
    "            \"value\": {\n",
    "              \"type\": \"MIDAS\",\n",
    "              \"value\": \"statement\",\n",
    "              \"confidence\": 0.9055033326148987,\n",
    "              \"_py_type\": \"cltl.dialogue_act_classification.api-DialogueAct\"\n",
    "            },\n",
    "            \"source\": \"MIDAS\",\n",
    "            \"timestamp\": 1760646835711,\n",
    "            \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27449c14-37be-4dca-8a17-11baccd5b18c",
   "metadata": {},
   "source": [
    "## Annotating utterances with Emotions\n",
    "\n",
    "Emotions cane be defined in many ways such as for facial expressions and for uttered text. Here we use a finetuned multilingual BERT model trained with the dataset ```GO``` created by Google that capture a broad range of emotions. This model is downloaded from Huggingface. \n",
    "\n",
    "**Reference**:\n",
    "\n",
    "```Demszky, D., Movshovitz-Attias, D., Ko, J., Cowen, A., Nemade, G., & Ravi, S. (2020). GoEmotions: A dataset of fine-grained emotions. arXiv preprint arXiv:2005.00547.```\n",
    "\n",
    "The annotator for the emotions has the following parameters that can be set:\n",
    "\n",
    "1. emissor = path to the emissor folder.\n",
    "2. scenario (optional) = the subfolder in the emissor folder that needs to processed. If omitted all subfolders are processed.\n",
    "3. model_name (optional) = the name of the model that is used to specificy the provenance of the annotation. If omitted GO is used as a a name.\n",
    "4. model_path (optional) = the name of the model on Huggingface or the path to a local model on your computer. If omitted the model on Huggingface is downloaded and used, which may take a bit of time the first time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "959d5cdd-8706-4943-915a-e0e067d96bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenarios:  ['14a1c27d-dfd2-465b-9ab2-90e9ea91d214']\n",
      "Processing scenario 14a1c27d-dfd2-465b-9ab2-90e9ea91d214\n"
     ]
    }
   ],
   "source": [
    "model = \"AnasAlokla/multilingual_go_emotions\"\n",
    "emotions.main(emissor=emissor,  scenario=scenario, model_path=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e80363-5918-47ff-b0f3-e87964ea1dcb",
   "metadata": {},
   "source": [
    "The next cell shows the JSON structure for the emotion annotation. Note that the 27 GO emotions are also mapped to the 7 basic Ekman emotions (joy, sadness, anger, surprise, disgust, fear, neutral) as well to sentiment: negative, positive and neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543b9cf-da8b-42a4-891c-afc4560d2fd5",
   "metadata": {},
   "source": [
    "```\n",
    "\"text\": \"I know. I have heard about you before\"\n",
    "  {\n",
    "    \"@context\": {...},\n",
    "    \"@type\": \"Mention\",\n",
    "    \"id\": \"6aab3fb9-9854-459f-a864-0b8a95c74bb0\",\n",
    "    \"segment\": [\n",
    "      {\n",
    "        \"@context\": {\n",
    "          \"Index\": \"https://emissor.org#Index\",\n",
    "          \"id\": \"@id\",\n",
    "          \"start\": \"https://emissor.org#start\",\n",
    "          \"stop\": \"https://emissor.org#stop\",\n",
    "          \"container_id\": {\n",
    "            \"@id\": \"https://emissor.org#container_id\",\n",
    "            \"@type\": \"@id\"\n",
    "          }\n",
    "        },\n",
    "        \"@type\": \"Index\",\n",
    "        \"container_id\": \"7d72105d-52d8-4ca1-b1e8-03d9f3d10d13\",\n",
    "        \"start\": 0,\n",
    "        \"stop\": 37,\n",
    "        \"_py_type\": \"emissor.representation.container-Index\"\n",
    "      }\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "      {\n",
    "        \"@context\": {...},\n",
    "        \"@type\": \"Annotation\",\n",
    "        \"type\": \"python-type:cltl.emotion_extraction.api.Emotion\",\n",
    "        \"value\": {\n",
    "          \"type\": \"GO\",\n",
    "          \"value\": \"approval\",\n",
    "          \"confidence\": 0.28517377376556396,\n",
    "          \"_py_type\": \"cltl.emotion_extraction.api-Emotion\"\n",
    "        },\n",
    "        \"source\": \"GO\",\n",
    "        \"timestamp\": 1760555549981,\n",
    "        \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "      },\n",
    "      {\n",
    "        \"@context\": {...},\n",
    "        \"@type\": \"Annotation\",\n",
    "        \"type\": \"python-type:cltl.emotion_extraction.api.Emotion\",\n",
    "        \"value\": {\n",
    "          \"type\": \"EKMAN\",\n",
    "          \"value\": \"joy\",\n",
    "          \"confidence\": 0.4837806256255135,\n",
    "          \"_py_type\": \"cltl.emotion_extraction.api-Emotion\"\n",
    "        },\n",
    "        \"source\": \"GO\",\n",
    "        \"timestamp\": 1760555549981,\n",
    "        \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "      }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364efdc7-3b58-472f-a933-1cb3e0edf0bd",
   "metadata": {},
   "source": [
    "## Annotating utterances with Likelihood\n",
    "\n",
    "Large Language Models are pretrained to predict masked words. We can use their top predictions and the probabilities for each to derive a likelihood score for any utterance given the preceding context. This likelihood score is added to each utterance in EMISSOR. If a word in the utterance is among the top predictions, we take its porbability score. If it does not occur, the score will be zero.\n",
    "\n",
    "The annotator for the Likehood has the following parameters that can be set:\n",
    "\n",
    "1. emissor = path to the emissor folder.\n",
    "2. scenario (optional) = the subfolder in the emissor folder that needs to processed. If omitted all subfolders are processed.\n",
    "3. model_name (optional) = the name of the model that is used to specificy the provenance of the annotation. If omitted GO is used as a a name.\n",
    "4. model_path (optional) = the name of the model on Huggingface or the path to a local model on your computer. If omitted ```google-bert/bert-base-multilingual-cased``` on Huggingface is downloaded and used, which may take a bit of time the first time.\n",
    "5. max_context (optional) = the maximum number of tokens from the preceeding context that is considered. If omitted set to 300.\n",
    "6. len_top_tokens (otional) = the number of top results that will be used to find the probability of the word in the utterance. If omitted set to 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18fafc23-dcb6-42be-9149-b3049b48bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the likelihood score using google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path google-bert/bert-base-multilingual-cased\n",
      "model_name google-bert/bert-base-multilingual-cased\n",
      "context_threshold 300\n",
      "top_results 20\n",
      "Processing scenarios:  ['14a1c27d-dfd2-465b-9ab2-90e9ea91d214']\n",
      "Processing scenario 14a1c27d-dfd2-465b-9ab2-90e9ea91d214\n"
     ]
    }
   ],
   "source": [
    "model = \"google-bert/bert-base-multilingual-cased\"\n",
    "context=300\n",
    "top_results = 20\n",
    "\n",
    "likelihood.main(emissor=emissor, scenario=scenario, model_path=model, model_name = model, max_context = context, len_top_tokens = top_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3132ac-b308-4ed4-82e9-f311959ee17b",
   "metadata": {},
   "source": [
    "The next cell shows how the Likelihood score is added as an annotation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3dc0f-4910-45ec-a7d1-1347c19b3e54",
   "metadata": {},
   "source": [
    "```\n",
    "\"text\": \"I know. I have heard about you before\"\n",
    "  {\n",
    "    \"@context\": {...},\n",
    "    \"@type\": \"Mention\",\n",
    "    \"id\": \"f85a06fb-c7a1-411f-9577-10d21db9f04b\",\n",
    "    \"segment\": [\n",
    "      {\n",
    "        \"@context\": {\n",
    "          \"Index\": \"https://emissor.org#Index\",\n",
    "          \"id\": \"@id\",\n",
    "          \"start\": \"https://emissor.org#start\",\n",
    "          \"stop\": \"https://emissor.org#stop\",\n",
    "          \"container_id\": {\n",
    "            \"@id\": \"https://emissor.org#container_id\",\n",
    "            \"@type\": \"@id\"\n",
    "          }\n",
    "        },\n",
    "        \"@type\": \"Index\",\n",
    "        \"container_id\": \"f410c57a-b295-4570-ad0f-767199129825\",\n",
    "        \"start\": 0,\n",
    "        \"stop\": 11,\n",
    "        \"_py_type\": \"emissor.representation.container-Index\"\n",
    "      }\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "      {\n",
    "        \"@context\": {\n",
    "          \"Annotation\": \"https://emissor.org#Annotation\",\n",
    "          \"id\": \"@id\",\n",
    "          \"type\": \"https://emissor.org#type\",\n",
    "          \"value\": \"https://emissor.org#value\",\n",
    "          \"source\": \"https://emissor.org#source\",\n",
    "          \"timestamp\": \"https://emissor.org#timestamp\"\n",
    "        },\n",
    "        \"@type\": \"Annotation\",\n",
    "        \"type\": \"Likelihood\",\n",
    "        \"value\": 0.28286586205164593,\n",
    "        \"source\": \"mBERT\",\n",
    "        \"timestamp\": 1760532618914,\n",
    "        \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18731595-66d8-47a8-b6cd-8f343c8180b9",
   "metadata": {},
   "source": [
    "## Annotating utterances with Linguistic structures\n",
    "\n",
    "The final annotation focuses on linguistic properties of the utterances. We use the [spaCy](https://spacy.io) software to get information about the part-of-speech of words used, the syntactic dependencies and the entities that are referred to. A special case are words that match categories of object recognition. Text references to these objects, as defined in the [COCO](https://cocodataset.org/#home) dataset for object recognition, can be interesting to study the physical grounding of the text.\n",
    "\n",
    "Since the this annotator uses spaCy it is necessary to download the spaCy language module. For English this would be throug the following command in a terminal in the same environment as this notebook:\n",
    "\n",
    "```python -m spacy download en_core_web_sm```\n",
    "\n",
    "It is also possible to use language modules for other languages provided by spaCy.\n",
    "\n",
    "The linguistic annotator can be called with the following paramters:\n",
    "\n",
    "1. emissor = path to the emissor folder.\n",
    "2. scenario = the subfolder in the emissor folder that needs to processed. If omitted all subfolders are processed.\n",
    "3. model  = the spaCy language model that is used to process the utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc50bf0-4ca3-4bc2-bc0f-2795a3b97803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenarios:  ['14a1c27d-dfd2-465b-9ab2-90e9ea91d214']\n",
      "Processing scenario 14a1c27d-dfd2-465b-9ab2-90e9ea91d214\n"
     ]
    }
   ],
   "source": [
    "spacy_language_model = \"en_core_web_sm\"\n",
    "mentions.main(emissor=emissor, scenario=scenario, model=spacy_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692d8a9-4a59-40e9-98f7-5f9354c95cf4",
   "metadata": {},
   "source": [
    "The next two cells show two annotations of mentions: a speaker reference and a part-of-speech tag for specific words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed270b73-a1f5-4e73-b3ed-b0a2a3cb28b0",
   "metadata": {},
   "source": [
    "```\n",
    "\"text\": \"I know. I have heard about you before\"\n",
    "{\n",
    "\"@context\": {...},\n",
    "\"@type\": \"Mention\",\n",
    "\"id\": \"74cca75b-4a94-49fb-b6f2-e4867357aee0\",\n",
    "\"segment\": [\n",
    "  {\n",
    "    \"@context\": {\n",
    "      \"Index\": \"https://emissor.org#Index\",\n",
    "      \"id\": \"@id\",\n",
    "      \"start\": \"https://emissor.org#start\",\n",
    "      \"stop\": \"https://emissor.org#stop\",\n",
    "      \"container_id\": {\n",
    "        \"@id\": \"https://emissor.org#container_id\",\n",
    "        \"@type\": \"@id\"\n",
    "      }\n",
    "    },\n",
    "    \"@type\": \"Index\",\n",
    "    \"container_id\": \"7d72105d-52d8-4ca1-b1e8-03d9f3d10d13\",\n",
    "    \"start\": 8,\n",
    "    \"stop\": 9,\n",
    "    \"_py_type\": \"emissor.representation.container-Index\"\n",
    "  }\n",
    "],\n",
    "\"annotations\": [\n",
    "  {\n",
    "    \"@context\": {\n",
    "      \"Annotation\": \"https://emissor.org#Annotation\",\n",
    "      \"id\": \"@id\",\n",
    "      \"type\": \"https://emissor.org#type\",\n",
    "      \"value\": \"https://emissor.org#value\",\n",
    "      \"source\": \"https://emissor.org#source\",\n",
    "      \"timestamp\": \"https://emissor.org#timestamp\"\n",
    "    },\n",
    "    \"@type\": \"Annotation\",\n",
    "    \"type\": \"Entity\",\n",
    "    \"value\": {\n",
    "      \"text\": \"I\",\n",
    "      \"type\": \"SPEAKER\",\n",
    "      \"segment\": [\n",
    "        0,\n",
    "        1\n",
    "      ],\n",
    "      \"_py_type\": \"cltl.nlp.api-Entity\"\n",
    "    },\n",
    "    \"source\": \"NLP\",\n",
    "    \"timestamp\": 1760553605282,\n",
    "    \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa32f3-6f0b-4d96-930a-603cc1e2e2e3",
   "metadata": {},
   "source": [
    "```\n",
    "\"text\": \"I know. I have heard about you before\"\n",
    "  {\n",
    "    \"@context\": {...},\n",
    "    \"@type\": \"Mention\",\n",
    "    \"id\": \"ed89a8be-eda3-4406-8b11-7c8676cfbe0d\",\n",
    "    \"segment\": [\n",
    "      {\n",
    "        \"@context\": {\n",
    "          \"Index\": \"https://emissor.org#Index\",\n",
    "          \"id\": \"@id\",\n",
    "          \"start\": \"https://emissor.org#start\",\n",
    "          \"stop\": \"https://emissor.org#stop\",\n",
    "          \"container_id\": {\n",
    "            \"@id\": \"https://emissor.org#container_id\",\n",
    "            \"@type\": \"@id\"\n",
    "          }\n",
    "        },\n",
    "        \"@type\": \"Index\",\n",
    "        \"container_id\": \"f410c57a-b295-4570-ad0f-767199129825\",\n",
    "        \"start\": 2,\n",
    "        \"stop\": 6,\n",
    "        \"_py_type\": \"emissor.representation.container-Index\"\n",
    "      }\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "      {\n",
    "        \"@context\": {\n",
    "          \"Annotation\": \"https://emissor.org#Annotation\",\n",
    "          \"id\": \"@id\",\n",
    "          \"type\": \"https://emissor.org#type\",\n",
    "          \"value\": \"https://emissor.org#value\",\n",
    "          \"source\": \"https://emissor.org#source\",\n",
    "          \"timestamp\": \"https://emissor.org#timestamp\"\n",
    "        },\n",
    "        \"@type\": \"Annotation\",\n",
    "        \"type\": \"Token\",\n",
    "        \"value\": {\n",
    "          \"text\": \"know\",\n",
    "          \"pos\": \"VERB\",\n",
    "          \"segment\": [\n",
    "            2,\n",
    "            6\n",
    "          ],\n",
    "          \"_py_type\": \"cltl.nlp.api-Token\"\n",
    "        },\n",
    "        \"source\": \"NLP\",\n",
    "        \"timestamp\": 1760533911733,\n",
    "        \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5ca0b-fd2a-45b5-919e-e181266c72f4",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bd1dc-56da-4ae9-a98b-284046c63689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotate",
   "language": "python",
   "name": "annotate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
